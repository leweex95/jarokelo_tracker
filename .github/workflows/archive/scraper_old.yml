name: Data scraper (old)

on:
  schedule:
    - cron: "0 22 * * *"
  workflow_dispatch:
    inputs:
      backend:
        description: 'Scraper backend to use (beautifulsoup/bs/selenium)'
        required: false
        default: 'beautifulsoup'
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
      headless:
        description: 'Run browser in headless mode'
        required: false
        default: 'true'
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
      buffer_size:
        description: 'Buffer size for comprehensive scraping (higher = faster but more memory)'
        required: false
        default: '200'
  workflow_call:
    inputs:
      backend:
        description: 'Scraper backend to use (beautifulsoup/bs/selenium)'
        required: false
        default: 'beautifulsoup'
        type: string
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
        type: string
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
        type: string
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
        type: string
      headless:
        description: 'Run browser in headless mode'
        required: false
        default: 'true'
        type: string
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
        type: string
      buffer_size:
        description: 'Buffer size for comprehensive scraping'
        required: false
        default: '200'
        type: string
    secrets:
      PAT_TOKEN:
        required: true

jobs:
  # Job 1: Scrape new entries from Jarokelo website
  comprehensive-scraping:
    runs-on: ubuntu-latest
    name: "Comprehensive scraping (new entries)"
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache Poetry packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry/cache
            ~/.cache/pypoetry/artifacts
          key: ${{ runner.os }}-poetry-${{ hashFiles('poetry.lock') }}
          restore-keys: ${{ runner.os }}-poetry-

      - name: Set up Python & Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade poetry==2.2.0
          poetry install --no-interaction

      - name: Count existing records
        id: count_before
        run: |
          RECORD_COUNT=$(find data/raw -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
          echo "records_before=$RECORD_COUNT" >> $GITHUB_OUTPUT
          echo "Existing records before scraping: $RECORD_COUNT"

      - name: Run comprehensive scraper
        run: |
          BACKEND="${{ github.event.inputs.backend || 'beautifulsoup' }}"
          HEADLESS="${{ github.event.inputs.headless || 'true' }}"
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          START_PAGE="${{ github.event.inputs.start_page || '1' }}"
          UNTIL_DATE="${{ github.event.inputs.until_date || '' }}"
          CONTINUE_SCRAPING="${{ github.event.inputs.continue_scraping || '' }}"

          echo "Running COMPREHENSIVE scraper for new entries..."
          echo "Data directory: $DATA_DIR"
          echo "Backend: $BACKEND"
          echo "Headless: $HEADLESS"
          echo "Start page: $START_PAGE"
          
          BUFFER_SIZE="${{ github.event.inputs.buffer_size || '200' }}"
          ARGS="--continue-scraping --data-dir data/raw --buffer-size $BUFFER_SIZE"
          if [ -n "$UNTIL_DATE" ]; then
            ARGS="$ARGS --until-date $UNTIL_DATE"
            echo "Until date: $UNTIL_DATE"
          fi
          if [ -n "$CONTINUE_SCRAPING" ]; then
            ARGS="$ARGS --continue-scraping"
            echo "Continue scraping: enabled"
          fi
          echo "Buffer size: $BUFFER_SIZE (for optimal performance)"
          echo "Async mode: enabled (8.6x performance boost with auto-fallback)"
          
          echo "Running: poetry run python ./scripts/scrape_data.py $ARGS"
          echo ""
          
          poetry run python ./scripts/scrape_data.py $ARGS

      - name: Count new records
        id: count_after
        run: |
          RECORD_COUNT=$(find data/raw -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
          echo "records_after=$RECORD_COUNT" >> $GITHUB_OUTPUT
          NEW_RECORDS=$((RECORD_COUNT - ${{ steps.count_before.outputs.records_before }}))
          echo "new_records=$NEW_RECORDS" >> $GITHUB_OUTPUT
          echo "Records after scraping: $RECORD_COUNT"
          echo "New records added: $NEW_RECORDS"

      - name: Check for changes
        id: check_changes
        run: |
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No new data to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "New data detected!"
            echo ""
            echo "📝 Changed files:"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit and push comprehensive scraping data
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          NEW_RECORDS="${{ steps.count_after.outputs.new_records }}"
          
          git add "$DATA_DIR"
          BUFFER_SIZE="${{ github.event.inputs.buffer_size || '200' }}"
          git commit -m "[Auto-commit] Comprehensive scraping: $NEW_RECORDS new records

          Scraped at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}
          Buffer size: $BUFFER_SIZE
          Total records: ${{ steps.count_after.outputs.records_after }}
          New records: $NEW_RECORDS
          
          [skip ci]"
          
          # Push with retry logic
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git pull --rebase
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:master && {
              echo "Successfully pushed comprehensive scraping data!"
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done

  # Job 2: Update status of existing records (runs in parallel with Job 1)
  status-updates:
    runs-on: ubuntu-latest
    name: "Status updates (old entries)"
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache Poetry packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry/cache
            ~/.cache/pypoetry/artifacts
          key: ${{ runner.os }}-poetry-${{ hashFiles('poetry.lock') }}
          restore-keys: ${{ runner.os }}-poetry-

      - name: Set up Python & Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade poetry==2.2.0
          poetry install --no-interaction

      - name: Count existing records
        id: count_records
        run: |
          RECORD_COUNT=$(find data/raw -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
          echo "existing_records=$RECORD_COUNT" >> $GITHUB_OUTPUT
          echo "Found $RECORD_COUNT existing records for status checking"

      - name: Run status update scraper
        run: |
          BACKEND="${{ github.event.inputs.backend || 'beautifulsoup' }}"
          HEADLESS="${{ github.event.inputs.headless || 'true' }}"
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          UNTIL_DATE="${{ github.event.inputs.until_date || '' }}"

          echo "Running STATUS UPDATE scraper..."
          echo "Data directory: $DATA_DIR"
          echo "Existing records: ${{ steps.count_records.outputs.existing_records }}"
          echo "Backend: $BACKEND"
          echo "Headless: $HEADLESS"
          
          ARGS="          ARGS: "--update-existing-status --data-dir data/raw""
          if [ -n "$UNTIL_DATE" ]; then
            ARGS="$ARGS --until-date $UNTIL_DATE"
            echo "Until date: $UNTIL_DATE"
          fi
          echo "Note: Buffering disabled for status updates (immediate writes)"
          echo "Async mode: enabled (8.6x performance boost with auto-fallback)"
          
          echo "Running: poetry run python ./scripts/scrape_data.py $ARGS"
          echo ""
          
          # Run the status update scraper
          poetry run python ./scripts/scrape_data.py $ARGS

      - name: Check for status changes
        id: check_changes
        run: |
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "ℹNo status changes detected"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "Status changes detected!"
            echo ""
            echo "Changed files:"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit and push status updates
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          
          # Count how many files were modified
          MODIFIED_COUNT=$(git diff --name-only "$DATA_DIR" | wc -l)
          
          git add "$DATA_DIR"
          git commit -m "[Auto-commit] Status updates: $MODIFIED_COUNT files updated

          Updated at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}
          Records checked: ${{ steps.count_records.outputs.existing_records }}
          Files modified: $MODIFIED_COUNT
          
          [skip ci]"
          
          # Push with retry logic
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git pull --rebase
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:master && {
              echo "Successfully pushed status updates!"
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done

  # Job 3: Report results after both jobs complete
  pipeline-summary:
    runs-on: ubuntu-latest
    name: "Pipeline Summary"
    needs: [comprehensive-scraping, status-updates]
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}
          ref: master

      - name: Report pipeline summary
        run: |
          echo "Járokelő Tracker Pipeline Summary"
          echo "==================================="
          echo "Pipeline mode: ${{ github.event_name == 'workflow_dispatch' && 'Manual' || 'Nightly Scheduled' }}"
          echo "Comprehensive scraping: ${{ needs.comprehensive-scraping.result || 'skipped' }}"
          echo "Status updates: ${{ needs.status-updates.result || 'skipped' }}"
          echo ""
          echo "Data directory structure:"
          ls -la data/raw/ || echo "No data directory found"
          echo ""
          echo "Total JSONL files:"
          find data/raw -name "*.jsonl" -type f | wc -l || echo "0"
          echo ""
          echo "Record counts by month:"
          for file in data/raw/*.jsonl; do
            if [ -f "$file" ]; then
              count=$(wc -l < "$file" 2>/dev/null || echo "0")
              echo "  $(basename "$file"): $count records"
            fi
          done
          echo ""
          echo "Pipeline completed at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "Repository: ${{ github.repository }}"
          echo "Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
