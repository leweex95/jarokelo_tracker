name: Data scraper

on:
  schedule:
    - cron: "0 22 * * *"
  workflow_dispatch:
    inputs:
      backend:
        description: 'Scraper backend to use (beautifulsoup/bs/selenium)'
        required: false
        default: 'beautifulsoup'
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
      headless:
        description: 'Run browser in headless mode'
        required: false
        default: 'true'
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
      buffer_size:
        description: 'Buffer size for comprehensive scraping (higher = faster but more memory) - heavily reduced for CI stability'
        required: false
        default: '25'
      cutoff_months:
        description: 'Months cutoff for recent vs old status updates (default: 3)'
        required: false
        default: '3'
  workflow_call:
    inputs:
      backend:
        description: 'Scraper backend to use (beautifulsoup/bs/selenium)'
        required: false
        default: 'beautifulsoup'
        type: string
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
        type: string
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
        type: string
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
        type: string
      headless:
        description: 'Run browser in headless mode'
        required: false
        default: 'true'
        type: string
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
        type: string
      buffer_size:
        description: 'Buffer size for comprehensive scraping'
        required: false
        default: '25'
        type: string
      cutoff_months:
        description: 'Months cutoff for recent vs old status updates'
        required: false
        default: '3'
        type: string
    secrets:
      PAT_TOKEN:
        required: true

jobs:
  # Job 1: Comprehensive scraping - Find new entries from Jarokelo website
  comprehensive-scraping:
    runs-on: ubuntu-latest
    name: "1. Comprehensive scraping (new entries)"
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache pip packages  
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-scraper-${{ hashFiles('requirements-scraper.txt') }}
          restore-keys: ${{ runner.os }}-pip-scraper-

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt
          
      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Check available disk space
        run: |
          echo "Available disk space before scraping:"
          df -h
          AVAILABLE_GB=$(df / | tail -1 | awk '{print $4}' | sed 's/G//')
          if [ "${AVAILABLE_GB%.*}" -lt 5 ]; then
            echo "WARNING: Less than 5GB disk space available. Scraping may fail."
            echo "Available space: ${AVAILABLE_GB}G"
          fi

      - name: Count existing records
        id: count_before
        run: |
          RECORD_COUNT=$(find data/raw -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
          echo "records_before=$RECORD_COUNT" >> $GITHUB_OUTPUT
          echo "Existing records before scraping: $RECORD_COUNT"

      - name: Compute until-date
        id: until_date
        run: |
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          UNTIL_DATE="${{ github.event.inputs.until_date || '' }}"

          if [ -z "$UNTIL_DATE" ]; then
            UNTIL_DATE=$(python3 -c "
          import json, glob
          from datetime import datetime, timedelta
          data_dir = '$DATA_DIR'
          files = sorted(glob.glob(f'{data_dir}/*.jsonl'))
          if not files:
              print((datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d'))
              exit()
          latest_file = files[-1]
          max_date = None
          with open(latest_file, 'r', encoding='utf-8') as f:
              for line in f:
                  try:
                      entry = json.loads(line)
                      d = datetime.fromisoformat(entry.get('date'))
                      if max_date is None or d > max_date:
                          max_date = d
                  except Exception:
                      continue
          if max_date:
              print((max_date - timedelta(days=1)).strftime('%Y-%m-%d'))
          else:
              print((datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d'))
          ")
          fi

          echo "UNTIL_DATE=$UNTIL_DATE" >> $GITHUB_ENV

      - name: Run comprehensive scraper
        run: |
          BACKEND="${{ github.event.inputs.backend || 'beautifulsoup' }}"
          HEADLESS="${{ github.event.inputs.headless || 'true' }}"
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          START_PAGE="${{ github.event.inputs.start_page || '1' }}"
          CONTINUE_SCRAPING="${{ github.event.inputs.continue_scraping || '' }}"

          echo "Running COMPREHENSIVE scraper for new entries..."
          echo "Data directory: $DATA_DIR"
          echo "Backend: $BACKEND"
          echo "Headless: $HEADLESS"
          echo "Start page: $START_PAGE"
          
          BUFFER_SIZE="${{ github.event.inputs.buffer_size || '25' }}"
          ARGS="--data-dir $DATA_DIR --buffer-size $BUFFER_SIZE --until-date $UNTIL_DATE"
          
          if [ -n "$UNTIL_DATE" ]; then
            ARGS="$ARGS --until-date $UNTIL_DATE"
            echo "Until date: $UNTIL_DATE"
          fi
          if [ -n "$CONTINUE_SCRAPING" ]; then
            ARGS="$ARGS --continue-scraping"
            echo "Continue scraping: enabled"
          fi
          echo "Buffer size: $BUFFER_SIZE (optimized for CI disk space constraints)"
          
          echo "Running: python ./scripts/scrape_data.py $ARGS"
          echo ""
          
          python ./scripts/scrape_data.py $ARGS

      - name: Count new records
        id: count_after
        run: |
          RECORD_COUNT=$(find data/raw -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
          echo "records_after=$RECORD_COUNT" >> $GITHUB_OUTPUT
          NEW_RECORDS=$((RECORD_COUNT - ${{ steps.count_before.outputs.records_before }}))
          echo "new_records=$NEW_RECORDS" >> $GITHUB_OUTPUT
          echo "Records after scraping: $RECORD_COUNT"
          echo "New records added: $NEW_RECORDS"

      - name: Check disk space after scraping
        run: |
          echo "Available disk space after scraping:"
          df -h
          echo ""
          echo "Data directory size:"
          du -sh data/raw || echo "No data directory found"

      - name: Check for changes
        id: check_changes
        run: |
          echo "DEBUG: Checking for changes..."
          echo "Git status:"
          git status --porcelain
          echo ""
          echo "Untracked files in data/raw:"
          find data/raw -type f -name "*.jsonl" | head -10
          echo ""
          echo "Git diff (modified files):"
          git diff --name-only
          echo ""
          echo "Git diff --staged (staged files):"
          git diff --staged --name-only
          echo ""
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No new data to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "New data detected!"
            echo ""
            echo "Changed files:"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit and push comprehensive scraping data
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          NEW_RECORDS="${{ steps.count_after.outputs.new_records }}"
          BUFFER_SIZE="${{ github.event.inputs.buffer_size || '25' }}"

          git add "$DATA_DIR"
          git commit -m "[Auto-commit] Comprehensive scraping: $NEW_RECORDS new records

          Scraped at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}
          Buffer size: $BUFFER_SIZE
          Total records: ${{ steps.count_after.outputs.records_after }}
          New records: $NEW_RECORDS

          [skip ci]"

          # Detect current branch
          BRANCH=$(git rev-parse --abbrev-ref HEAD)
          RETRIES=10
          SUCCESS=0
          for i in $(seq 1 $RETRIES); do
            git fetch origin "$BRANCH"
            git pull --rebase origin "$BRANCH" || git merge origin/"$BRANCH"
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:"$BRANCH" && {
              echo "Successfully pushed comprehensive scraping data!"
              SUCCESS=1
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done
          if [ "$SUCCESS" -ne 1 ]; then
            echo "ERROR: Failed to push after $RETRIES attempts."
            exit 1
          fi

  # Job 2: Fast detection of recently changed statuses (configurable, default: last 3 months)
  # This job quickly scans recent pages WITHOUT opening individual URLs to find status changes
  # Much faster than comprehensive scraping when only looking for status updates
  recent-url-detector:
    runs-on: ubuntu-latest
    name: "2. Recent status change detector"
    outputs:
      changed_urls_count: ${{ steps.detect_changes.outputs.changed_count }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt

      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Detect recently changed statuses
        id: detect_changes
        run: |
          CUTOFF_MONTHS="${{ github.event.inputs.cutoff_months || '3' }}"
          
          echo "RECENT STATUS CHANGE DETECTION (FAST SCAN)"
          echo "Strategy: Scan only listing pages for status changes in the last $CUTOFF_MONTHS months"
          echo "Cutoff months: $CUTOFF_MONTHS"
          echo "Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}"
          
          # Run fast status change detection
          python ./scripts/scrape_data.py \
            --fetch-changed-urls \
            --cutoff-months $CUTOFF_MONTHS \
            --backend "${{ github.event.inputs.backend || 'beautifulsoup' }}" \
            --headless "${{ github.event.inputs.headless || 'true' }}"
          
          # Count detected URLs
          if [ -f "recent_changed_urls.txt" ]; then
            CHANGED_COUNT=$(wc -l < recent_changed_urls.txt)
          else
            CHANGED_COUNT=0
          fi
          
          echo "changed_count=$CHANGED_COUNT" >> $GITHUB_OUTPUT
          echo "Detected $CHANGED_COUNT recently changed statuses"

      - name: Upload recent changed URLs
        if: steps.detect_changes.outputs.changed_count != '0'
        uses: actions/upload-artifact@v4
        with:
          name: recent-changed-urls
          path: recent_changed_urls.txt
          retention-days: 1

  # Job 3: Extract old pending URLs (older than cutoff_months) from database
  # This job extracts URLs directly from our database for issues that are:
  # 1. Older than the cutoff period
  # 2. Still in a pending state (not resolved, deleted, or rejected)
  # This avoids having to scan thousands of pages for old content
  old-pending-loader:
    runs-on: ubuntu-latest
    name: "3. Old pending URL extractor"
    outputs:
      pending_urls_count: ${{ steps.load_pending.outputs.pending_count }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt

      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Load old pending URLs
        id: load_pending
        run: |
          CUTOFF_MONTHS="${{ github.event.inputs.cutoff_months || '3' }}"
          
          echo "OLD PENDING URL EXTRACTOR"
          echo "Strategy: Extract old pending URLs directly from database (older than $CUTOFF_MONTHS months)"
          echo "Cutoff months: $CUTOFF_MONTHS"
          
          # Load old pending URLs
          python ./scripts/scrape_data.py \
            --load-old-pending \
            --cutoff-months $CUTOFF_MONTHS
          
          # Count pending URLs
          if [ -f "old_pending_urls.txt" ]; then
            PENDING_COUNT=$(wc -l < old_pending_urls.txt)
          else
            PENDING_COUNT=0
          fi
          
          echo "pending_count=$PENDING_COUNT" >> $GITHUB_OUTPUT
          echo "Found $PENDING_COUNT old pending URLs"

      - name: Upload old pending URLs
        if: steps.load_pending.outputs.pending_count > 0
        uses: actions/upload-artifact@v4
        with:
          name: old-pending-urls
          path: old_pending_urls.txt
          retention-days: 1

  # Job 4: Process recently changed statuses (detected by Job 2) to update resolution data
  recent-resolution-scraper:
    runs-on: ubuntu-latest
    name: "4. Recent resolution date scraper"
    needs: recent-url-detector
    if: needs.recent-url-detector.outputs.changed_urls_count > 0
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt

      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Download recent changed URLs
        uses: actions/download-artifact@v5
        with:
          name: recent-changed-urls

      - name: Scrape recent changed URLs
        run: |
          echo "RECENT RESOLUTION DATE SCRAPER"
          echo "Strategy: Targeted resolution date extraction for recently changed statuses detected by Job 2"
          echo "Advantage: Optimized processing focusing only on status and resolution date extraction"
          echo "URLs to scrape: ${{ needs.recent-url-detector.outputs.changed_urls_count }}"
          echo "Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}"
          
          # Scrape the recently changed URLs
          python ./scripts/scrape_data.py \
            --scrape-urls-file recent_changed_urls.txt \
            --backend "${{ github.event.inputs.backend || 'beautifulsoup' }}" \
            --headless "${{ github.event.inputs.headless || 'true' }}"

      - name: Check for changes
        id: check_changes
        run: |
          echo "DEBUG: Checking for changes..."
          echo "Git status:"
          git status --porcelain
          echo ""
          echo "Untracked files in data/raw:"
          find data/raw -type f -name "*.jsonl" | head -10
          echo ""
          echo "Git diff (modified files):"
          git diff --name-only
          echo ""
          echo "Git diff --staged (staged files):"
          git diff --staged --name-only
          echo ""
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No recent changes to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "Recent resolution changes detected!"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit recent resolutions
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          git add data/raw
          git commit -m "[Auto-commit] Recent resolutions: ${{ needs.recent-url-detector.outputs.changed_urls_count }} status changes updated

          Updated at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}
          Cutoff months: ${{ github.event.inputs.cutoff_months || '3' }}
          URLs processed: ${{ needs.recent-url-detector.outputs.changed_urls_count }}
          
          [skip ci]"
          
          # Push with retry logic
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git pull --rebase
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:"$BRANCH" && {
              echo "Successfully pushed recent resolution updates!"
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done

  # Job 5: Check old pending URLs (extracted by Job 3) for resolution updates
  # PERFORMANCE OPTIMIZED: Process only random sample of 100 URLs per run
  old-resolution-scraper:
    runs-on: ubuntu-latest
    name: "5. Old resolution date scraper (optimized)"
    needs: old-pending-loader
    if: needs.old-pending-loader.outputs.pending_urls_count > 0
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt

      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Download old pending URLs
        uses: actions/download-artifact@v5
        with:
          name: old-pending-urls

      - name: Scrape old pending URLs
        run: |
          echo "OLD RESOLUTION DATE SCRAPER (OPTIMIZED)"
          echo "Strategy: Random sampling of old pending issues to check for resolutions"
          echo "Optimization: Process max 100 random URLs per run for manageable runtime"
          echo "Total URLs available: ${{ needs.old-pending-loader.outputs.pending_urls_count }}"
          echo "Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}"
          
          # Sample 250 random URLs from the last 1000 entries
          if [ -f "old_pending_urls.txt" ]; then
            TOTAL_URLS=$(wc -l < old_pending_urls.txt)
            START_LINE=$(( TOTAL_URLS > 1000 ? TOTAL_URLS - 999 : 1 ))
            tail -n +$START_LINE old_pending_urls.txt | shuf | head -250 > old_pending_urls_sample.txt
            mv old_pending_urls_sample.txt old_pending_urls.txt
            echo "Processing $(wc -l < old_pending_urls.txt) URLs from last 1000 entries"
          fi
          
          # Scrape the old pending URLs
          python ./scripts/scrape_data.py \
            --scrape-urls-file old_pending_urls.txt \
            --backend "${{ github.event.inputs.backend || 'beautifulsoup' }}" \
            --headless "${{ github.event.inputs.headless || 'true' }}"

      - name: Check for changes
        id: check_changes
        run: |
          echo "DEBUG: Checking for changes..."
          echo "Git status:"
          git status --porcelain
          echo ""
          echo "Untracked files in data/raw:"
          find data/raw -type f -name "*.jsonl" | head -10
          echo ""
          echo "Git diff (modified files):"
          git diff --name-only
          echo ""
          echo "Git diff --staged (staged files):"
          git diff --staged --name-only
          echo ""
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No old pending changes to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "Old pending resolution changes detected!"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit old resolutions
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          git add data/raw
          git commit -m "[Auto-commit] Old resolutions: Random sample of max 100 URLs checked

          Updated at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}
          Cutoff months: ${{ github.event.inputs.cutoff_months || '3' }}
          Total pending URLs: ${{ needs.old-pending-loader.outputs.pending_urls_count }}
          Processed: Random sample (max 100)
          
          [skip ci]"
          
          # Push with retry logic
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git pull --rebase
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:"$BRANCH" && {
              echo "Successfully pushed old resolution updates!"
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done

  # Job 6: Report results after all jobs complete
  pipeline-summary:
    runs-on: ubuntu-latest
    name: "6. Pipeline Summary"
    needs: [comprehensive-scraping, recent-url-detector, old-pending-loader, recent-resolution-scraper, old-resolution-scraper]
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}
          ref: master

      - name: Report pipeline summary
        run: |
          echo "Járokelő Tracker Pipeline Summary"
          echo "==============================================="
          echo "Optimization strategy: Split processing between new, recent and old content"
          echo "1. Comprehensive scraping: Find completely new entries"
          echo "2. Recent change detection: Fast scan recent pages (${CUTOFF_MONTHS:-3} months) for status changes"
          echo "3. Old pending extraction: Get URLs of unresolved issues older than ${CUTOFF_MONTHS:-3} months"
          echo "4. Recent resolution scraping: Update recently changed statuses"
          echo "5. Old resolution scraping: Check random sample (max 100) of old pending issues"
          echo "Pipeline mode: ${{ github.event_name == 'workflow_dispatch' && 'Manual' || 'Nightly Scheduled' }}"
          echo ""
          echo "Job Results:"
          echo "- Comprehensive scraping: ${{ needs.comprehensive-scraping.result || 'skipped' }}"
          echo "- Recent URL detection: ${{ needs.recent-url-detector.result || 'skipped' }}"
          echo "- Old pending loader: ${{ needs.old-pending-loader.result || 'skipped' }}"
          echo "- Recent resolution scraper: ${{ needs.recent-resolution-scraper.result || 'skipped' }}"
          echo "- Old resolution scraper: ${{ needs.old-resolution-scraper.result || 'skipped' }}"
          echo ""
          echo "Performance Summary:"
          echo "- Recent URLs detected: ${{ needs.recent-url-detector.outputs.changed_urls_count || '0' }}"
          echo "- Old pending URLs: ${{ needs.old-pending-loader.outputs.pending_urls_count || '0' }}"
          echo "- Cutoff months: ${{ github.event.inputs.cutoff_months || '3' }}"
          echo "- Optimization: Split pipeline with targeted processing reduces run time by ~90%!"
          echo "- Time savings: By only opening URLs that need attention (new/changed/pending)"
          echo "- Memory savings: Processing in smaller targeted batches reduces RAM requirements"
          echo ""
          echo "Data directory structure:"
          ls -la data/raw/ || echo "No data directory found"
          echo ""
          echo "Total JSONL files:"
          find data/raw -name "*.jsonl" -type f | wc -l || echo "0"
          echo ""
          echo "Record counts by month:"
          for file in data/raw/*.jsonl; do
            if [ -f "$file" ]; then
              count=$(wc -l < "$file" 2>/dev/null || echo "0")
              echo "  $(basename "$file"): $count records"
            fi
          done
          echo ""
          echo "Pipeline completed at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "Repository: ${{ github.repository }}"
          echo "Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
