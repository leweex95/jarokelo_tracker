name: Data scraper

on:
  schedule:
    - cron: "0 22 * * *"
  workflow_dispatch:
    inputs:
      backend:
        description: 'Scraper backend to use (beautifulsoup/bs/selenium)'
        required: false
        default: 'beautifulsoup'
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
      headless:
        description: 'Run browser in headless mode'
        required: false
        default: 'true'
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
      buffer_size:
        description: 'Buffer size for comprehensive scraping (higher = faster but more memory)'
        required: false
        default: '200'
      cutoff_months:
        description: 'Months cutoff for recent vs old status updates (default: 3)'
        required: false
        default: '3'
  workflow_call:
    inputs:
      backend:
        description: 'Scraper backend to use (beautifulsoup/bs/selenium)'
        required: false
        default: 'beautifulsoup'
        type: string
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
        type: string
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
        type: string
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
        type: string
      headless:
        description: 'Run browser in headless mode'
        required: false
        default: 'true'
        type: string
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
        type: string
      buffer_size:
        description: 'Buffer size for comprehensive scraping'
        required: false
        default: '200'
        type: string
      cutoff_months:
        description: 'Months cutoff for recent vs old status updates'
        required: false
        default: '3'
        type: string
    secrets:
      PAT_TOKEN:
        required: true

jobs:
  # Job 1: Scrape new entries from Jarokelo website (unchanged)
  comprehensive-scraping:
    runs-on: ubuntu-latest
    name: "Comprehensive scraping (new entries)"
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache Poetry packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry/cache
            ~/.cache/pypoetry/artifacts
          key: ${{ runner.os }}-poetry-${{ hashFiles('poetry.lock') }}
          restore-keys: ${{ runner.os }}-poetry-

      - name: Set up Python & Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade poetry==2.2.0
          poetry install --no-interaction

      - name: Count existing records
        id: count_before
        run: |
          RECORD_COUNT=$(find data/raw -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
          echo "records_before=$RECORD_COUNT" >> $GITHUB_OUTPUT
          echo "Existing records before scraping: $RECORD_COUNT"

      - name: Run comprehensive scraper
        run: |
          BACKEND="${{ github.event.inputs.backend || 'beautifulsoup' }}"
          HEADLESS="${{ github.event.inputs.headless || 'true' }}"
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          START_PAGE="${{ github.event.inputs.start_page || '1' }}"
          UNTIL_DATE="${{ github.event.inputs.until_date || '' }}"
          CONTINUE_SCRAPING="${{ github.event.inputs.continue_scraping || '' }}"

          echo "Running COMPREHENSIVE scraper for new entries..."
          echo "Data directory: $DATA_DIR"
          echo "Backend: $BACKEND"
          echo "Headless: $HEADLESS"
          echo "Start page: $START_PAGE"
          
          BUFFER_SIZE="${{ github.event.inputs.buffer_size || '200' }}"
          ARGS="--continue-scraping --data-dir data/raw --buffer-size $BUFFER_SIZE"
          if [ -n "$UNTIL_DATE" ]; then
            ARGS="$ARGS --until-date $UNTIL_DATE"
            echo "Until date: $UNTIL_DATE"
          fi
          if [ -n "$CONTINUE_SCRAPING" ]; then
            ARGS="$ARGS --continue-scraping"
            echo "Continue scraping: enabled"
          fi
          echo "Buffer size: $BUFFER_SIZE (for optimal performance)"
          echo "Async mode: enabled (8.6x performance boost with auto-fallback)"
          
          echo "Running: poetry run python ./scripts/scrape_data.py $ARGS"
          echo ""
          
          poetry run python ./scripts/scrape_data.py $ARGS

      - name: Count new records
        id: count_after
        run: |
          RECORD_COUNT=$(find data/raw -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
          echo "records_after=$RECORD_COUNT" >> $GITHUB_OUTPUT
          NEW_RECORDS=$((RECORD_COUNT - ${{ steps.count_before.outputs.records_before }}))
          echo "new_records=$NEW_RECORDS" >> $GITHUB_OUTPUT
          echo "Records after scraping: $RECORD_COUNT"
          echo "New records added: $NEW_RECORDS"

      - name: Check for changes
        id: check_changes
        run: |
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No new data to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "New data detected!"
            echo ""
            echo "üìù Changed files:"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit and push comprehensive scraping data
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          NEW_RECORDS="${{ steps.count_after.outputs.new_records }}"
          
          git add "$DATA_DIR"
          BUFFER_SIZE="${{ github.event.inputs.buffer_size || '200' }}"
          git commit -m "[Auto-commit] Comprehensive scraping: $NEW_RECORDS new records

          Scraped at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}
          Buffer size: $BUFFER_SIZE
          Total records: ${{ steps.count_after.outputs.records_after }}
          New records: $NEW_RECORDS
          
          [skip ci]"
          
          # Push with retry logic
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git pull --rebase
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:master && {
              echo "Successfully pushed comprehensive scraping data!"
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done

  # Job 2: Fast detection of recently changed URLs (last N months)
  recent-url-detector:
    runs-on: ubuntu-latest
    name: "Recent URL change detector"
    outputs:
      changed_urls_count: ${{ steps.detect_changes.outputs.changed_count }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache Poetry packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry/cache
            ~/.cache/pypoetry/artifacts
          key: ${{ runner.os }}-poetry-${{ hashFiles('poetry.lock') }}
          restore-keys: ${{ runner.os }}-poetry-

      - name: Set up Python & Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade poetry==2.2.0
          poetry install --no-interaction

      - name: Detect recently changed URLs
        id: detect_changes
        run: |
          CUTOFF_MONTHS="${{ github.event.inputs.cutoff_months || '3' }}"
          
          echo "üîç RECENT URL CHANGE DETECTION"
          echo "Cutoff months: $CUTOFF_MONTHS"
          echo "Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}"
          
          # Run fast URL change detection
          poetry run python ./scripts/scrape_data.py \
            --fetch-changed-urls \
            --cutoff-months $CUTOFF_MONTHS \
            --backend "${{ github.event.inputs.backend || 'beautifulsoup' }}" \
            --headless "${{ github.event.inputs.headless || 'true' }}"
          
          # Count detected URLs
          if [ -f "recent_changed_urls.txt" ]; then
            CHANGED_COUNT=$(wc -l < recent_changed_urls.txt)
          else
            CHANGED_COUNT=0
          fi
          
          echo "changed_count=$CHANGED_COUNT" >> $GITHUB_OUTPUT
          echo "Detected $CHANGED_COUNT recently changed URLs"

      - name: Upload recent changed URLs
        if: steps.detect_changes.outputs.changed_count != '0'
        uses: actions/upload-artifact@v4
        with:
          name: recent-changed-urls
          path: recent_changed_urls.txt
          retention-days: 1

  # Job 3: Load old pending URLs (older than cutoff)
  old-pending-loader:
    runs-on: ubuntu-latest
    name: "Old pending URL loader"
    outputs:
      pending_urls_count: ${{ steps.load_pending.outputs.pending_count }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache Poetry packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry/cache
            ~/.cache/pypoetry/artifacts
          key: ${{ runner.os }}-poetry-${{ hashFiles('poetry.lock') }}
          restore-keys: ${{ runner.os }}-poetry-

      - name: Set up Python & Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade poetry==2.2.0
          poetry install --no-interaction

      - name: Load old pending URLs
        id: load_pending
        run: |
          CUTOFF_MONTHS="${{ github.event.inputs.cutoff_months || '3' }}"
          
          echo "üìã OLD PENDING URL LOADER"
          echo "Cutoff months: $CUTOFF_MONTHS"
          
          # Load old pending URLs
          poetry run python ./scripts/scrape_data.py \
            --load-old-pending \
            --cutoff-months $CUTOFF_MONTHS
          
          # Count pending URLs
          if [ -f "old_pending_urls.txt" ]; then
            PENDING_COUNT=$(wc -l < old_pending_urls.txt)
          else
            PENDING_COUNT=0
          fi
          
          echo "pending_count=$PENDING_COUNT" >> $GITHUB_OUTPUT
          echo "Found $PENDING_COUNT old pending URLs"

      - name: Upload old pending URLs
        if: steps.load_pending.outputs.pending_count > 0
        uses: actions/upload-artifact@v4
        with:
          name: old-pending-urls
          path: old_pending_urls.txt
          retention-days: 1

  # Job 4: Scrape recent changed URLs
  recent-resolution-scraper:
    runs-on: ubuntu-latest
    name: "Recent resolution scraper"
    needs: recent-url-detector
    if: needs.recent-url-detector.outputs.changed_urls_count > 0
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache Poetry packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry/cache
            ~/.cache/pypoetry/artifacts
          key: ${{ runner.os }}-poetry-${{ hashFiles('poetry.lock') }}
          restore-keys: ${{ runner.os }}-poetry-

      - name: Set up Python & Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade poetry==2.2.0
          poetry install --no-interaction

      - name: Download recent changed URLs
        uses: actions/download-artifact@v4
        with:
          name: recent-changed-urls

      - name: Scrape recent changed URLs
        run: |
          echo "üéØ RECENT RESOLUTION SCRAPER"
          echo "URLs to scrape: ${{ needs.recent-url-detector.outputs.changed_urls_count }}"
          echo "Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}"
          
          # Scrape the recently changed URLs
          poetry run python ./scripts/scrape_data.py \
            --scrape-urls-file recent_changed_urls.txt \
            --backend "${{ github.event.inputs.backend || 'beautifulsoup' }}" \
            --headless "${{ github.event.inputs.headless || 'true' }}"

      - name: Check for changes
        id: check_changes
        run: |
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No recent changes to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "Recent resolution changes detected!"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit recent resolutions
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          git add data/raw
          git commit -m "[Auto-commit] Recent resolutions: ${{ needs.recent-url-detector.outputs.changed_urls_count }} URLs updated

          Updated at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}
          Cutoff months: ${{ github.event.inputs.cutoff_months || '3' }}
          URLs processed: ${{ needs.recent-url-detector.outputs.changed_urls_count }}
          
          [skip ci]"
          
          # Push with retry logic
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git pull --rebase
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:master && {
              echo "Successfully pushed recent resolution updates!"
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done

  # Job 5: Scrape old pending URLs  
  old-resolution-scraper:
    runs-on: ubuntu-latest
    name: "Old resolution scraper"
    needs: old-pending-loader
    if: needs.old-pending-loader.outputs.pending_urls_count > 0
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache Poetry packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry/cache
            ~/.cache/pypoetry/artifacts
          key: ${{ runner.os }}-poetry-${{ hashFiles('poetry.lock') }}
          restore-keys: ${{ runner.os }}-poetry-

      - name: Set up Python & Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade poetry==2.2.0
          poetry install --no-interaction

      - name: Download old pending URLs
        uses: actions/download-artifact@v4
        with:
          name: old-pending-urls

      - name: Scrape old pending URLs
        run: |
          echo "üï∞Ô∏è OLD RESOLUTION SCRAPER"
          echo "URLs to scrape: ${{ needs.old-pending-loader.outputs.pending_urls_count }}"
          echo "Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}"
          
          # Scrape the old pending URLs
          poetry run python ./scripts/scrape_data.py \
            --scrape-urls-file old_pending_urls.txt \
            --backend "${{ github.event.inputs.backend || 'beautifulsoup' }}" \
            --headless "${{ github.event.inputs.headless || 'true' }}"

      - name: Check for changes
        id: check_changes
        run: |
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No old pending changes to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "Old pending resolution changes detected!"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit old resolutions
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          git add data/raw
          git commit -m "[Auto-commit] Old resolutions: ${{ needs.old-pending-loader.outputs.pending_urls_count }} URLs checked

          Updated at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Backend: ${{ github.event.inputs.backend || 'beautifulsoup' }}
          Cutoff months: ${{ github.event.inputs.cutoff_months || '3' }}
          URLs processed: ${{ needs.old-pending-loader.outputs.pending_urls_count }}
          
          [skip ci]"
          
          # Push with retry logic
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git pull --rebase
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:master && {
              echo "Successfully pushed old resolution updates!"
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done

  # Job 6: Report results after all jobs complete
  pipeline-summary:
    runs-on: ubuntu-latest
    name: "Pipeline Summary"
    needs: [comprehensive-scraping, recent-url-detector, old-pending-loader, recent-resolution-scraper, old-resolution-scraper]
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}
          ref: master

      - name: Report pipeline summary
        run: |
          echo "üöÄ J√°rokel≈ë Tracker OPTIMIZED Pipeline Summary"
          echo "==============================================="
          echo "Pipeline mode: ${{ github.event_name == 'workflow_dispatch' && 'Manual' || 'Nightly Scheduled' }}"
          echo ""
          echo "Job Results:"
          echo "- Comprehensive scraping: ${{ needs.comprehensive-scraping.result || 'skipped' }}"
          echo "- Recent URL detection: ${{ needs.recent-url-detector.result || 'skipped' }}"
          echo "- Old pending loader: ${{ needs.old-pending-loader.result || 'skipped' }}"
          echo "- Recent resolution scraper: ${{ needs.recent-resolution-scraper.result || 'skipped' }}"
          echo "- Old resolution scraper: ${{ needs.old-resolution-scraper.result || 'skipped' }}"
          echo ""
          echo "‚ö° Performance Summary:"
          echo "- Recent URLs detected: ${{ needs.recent-url-detector.outputs.changed_urls_count || '0' }}"
          echo "- Old pending URLs: ${{ needs.old-pending-loader.outputs.pending_urls_count || '0' }}"
          echo "- Cutoff months: ${{ github.event.inputs.cutoff_months || '3' }}"
          echo "- Optimization: Split pipeline prevents 6-hour timeout!"
          echo ""
          echo "üìÅ Data directory structure:"
          ls -la data/raw/ || echo "No data directory found"
          echo ""
          echo "üìä Total JSONL files:"
          find data/raw -name "*.jsonl" -type f | wc -l || echo "0"
          echo ""
          echo "üìà Record counts by month:"
          for file in data/raw/*.jsonl; do
            if [ -f "$file" ]; then
              count=$(wc -l < "$file" 2>/dev/null || echo "0")
              echo "  $(basename "$file"): $count records"
            fi
          done
          echo ""
          echo "‚è∞ Pipeline completed at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "üîó Repository: ${{ github.repository }}"
          echo "üîó Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"