name: Data scraper

on:
  schedule:
    - cron: "0 22 * * *"
  workflow_dispatch:
    inputs:
      backend:
        description: 'Scraper backend to use (beautifulsoup/bs/selenium)'
        required: false
        default: 'beautifulsoup'
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
      headless:
        description: 'Run browser in headless mode'
        required: false
        default: 'true'
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
  workflow_call:
    inputs:
      backend:
        description: 'Scraper backend to use (beautifulsoup/bs/selenium)'
        required: false
        default: 'beautifulsoup'
        type: string
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
        type: string
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
        type: string
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
        type: string
      headless:
        description: 'Run browser in headless mode'
        required: false
        default: 'true'
        type: string
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
        type: string
    secrets:
      PAT_TOKEN:
        required: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache Poetry packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry/cache
            ~/.cache/pypoetry/artifacts
          key: ${{ runner.os }}-poetry-${{ hashFiles('poetry.lock') }}
          restore-keys: ${{ runner.os }}-poetry-

      - name: Set up Python & Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade poetry==2.2.0
          poetry install --no-interaction --no-root

      - name: Run scraper
        run: |
          BACKEND="${{ github.event.inputs.backend || 'beautifulsoup' }}"
          HEADLESS="${{ github.event.inputs.headless || 'true' }}"
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          START_PAGE="${{ github.event.inputs.start_page || '1' }}"
          UNTIL_DATE="${{ github.event.inputs.until_date || '' }}"
          CONTINUE_SCRAPING="${{ github.event.inputs.continue_scraping || '' }}"

          ARGS="--backend $BACKEND --headless $HEADLESS --data-dir $DATA_DIR --start-page $START_PAGE"
          if [ -n "$UNTIL_DATE" ]; then
            ARGS="$ARGS --until-date $UNTIL_DATE"
          fi
          if [ -n "$CONTINUE_SCRAPING" ]; then
            ARGS="$ARGS --continue-scraping"
          fi
          echo "Running: poetry run python ./scripts/scrape_data.py $ARGS"
          poetry run python ./scripts/scrape_data.py $ARGS

      - name: Configure Git
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit and push scraped data
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          DATA_DIR="${{ github.event.inputs.data_dir }}"
          if [ -z "$DATA_DIR" ]; then
            DATA_DIR="data/raw"
          fi
          git add "$DATA_DIR"
          git diff --quiet && git diff --staged --quiet || git commit -m "[Auto-commit] Add new scraped data [skip ci]"
          
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git pull --rebase
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:master && break || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done
