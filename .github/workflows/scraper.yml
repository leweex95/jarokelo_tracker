name: Data scraper

on:
  schedule:
    - cron: "0 22 * * *"
  workflow_dispatch:
    inputs:
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
      buffer_size:
        description: 'Buffer size for comprehensive scraping (higher = faster but more memory) - heavily reduced for CI stability'
        required: false
        default: '25'
      cutoff_months:
        description: 'Months cutoff for recent vs old status updates (default: 3)'
        required: false
        default: '3'
  workflow_call:
    inputs:
      data_dir:
        description: 'Target data directory'
        required: false
        default: 'data/raw'
        type: string
      start_page:
        description: 'Start page number'
        required: false
        default: '1'
        type: string
      until_date:
        description: 'Scrape until this date (YYYY-MM-DD)'
        required: false
        default: ''
        type: string
      continue_scraping:
        description: 'Continue scraping from last saved state'
        required: false
        default: ''
        type: string
      buffer_size:
        description: 'Buffer size for comprehensive scraping'
        required: false
        default: '25'
        type: string
      cutoff_months:
        description: 'Months cutoff for recent vs old status updates'
        required: false
        default: '3'
        type: string
    secrets:
      PAT_TOKEN:
        required: true

jobs:
  # Job 1: Full scraping of new entries
  new-entries-scraping:
    runs-on: ubuntu-latest
    name: "1. Comprehensive scraping"
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Cache pip packages  
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-scraper-${{ hashFiles('requirements-scraper.txt') }}
          restore-keys: ${{ runner.os }}-pip-scraper-

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt
          
      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Count existing records
        id: count_before
        run: |
          RECORD_COUNT=$(find data/raw -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
          echo "records_before=$RECORD_COUNT" >> $GITHUB_OUTPUT
          echo "Existing records before scraping: $RECORD_COUNT"

      - name: Compute until-date
        id: until_date
        run: |
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          UNTIL_DATE="${{ github.event.inputs.until_date || '' }}"

          if [ -z "$UNTIL_DATE" ]; then
            UNTIL_DATE=$(python3 -c "
          import json, glob
          from datetime import datetime, timedelta
          data_dir = '$DATA_DIR'
          files = sorted(glob.glob(f'{data_dir}/*.jsonl'))
          if not files:
              print((datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d'))
              exit()
          latest_file = files[-1]
          max_date = None
          with open(latest_file, 'r', encoding='utf-8') as f:
              for line in f:
                  try:
                      entry = json.loads(line)
                      d = datetime.fromisoformat(entry.get('date'))
                      if max_date is None or d > max_date:
                          max_date = d
                  except Exception:
                      continue
          if max_date:
              print((max_date - timedelta(days=1)).strftime('%Y-%m-%d'))
          else:
              print((datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d'))
          ")
          fi

          echo "UNTIL_DATE=$UNTIL_DATE" >> $GITHUB_ENV

      - name: Run comprehensive scraper
        run: |
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          START_PAGE="${{ github.event.inputs.start_page || '1' }}"
          CONTINUE_SCRAPING="${{ github.event.inputs.continue_scraping || '' }}"

          echo "Running COMPREHENSIVE scraper for new entries..."
          echo "Data directory: $DATA_DIR"
          echo "Start page: $START_PAGE"
          
          BUFFER_SIZE="${{ github.event.inputs.buffer_size || '25' }}"
          ARGS="--data-dir $DATA_DIR --buffer-size $BUFFER_SIZE --until-date $UNTIL_DATE"
          
          if [ -n "$UNTIL_DATE" ]; then
            ARGS="$ARGS --until-date $UNTIL_DATE"
            echo "Until date: $UNTIL_DATE"
          fi
          if [ -n "$CONTINUE_SCRAPING" ]; then
            ARGS="$ARGS --continue-scraping"
            echo "Continue scraping: enabled"
          fi
          echo "Buffer size: $BUFFER_SIZE (optimized for CI disk space constraints)"
          
          echo "Running: python ./scripts/scrape_data.py $ARGS"
          echo ""
          
          python ./scripts/scrape_data.py $ARGS

      - name: Count new records
        id: count_after
        run: |
          RECORD_COUNT=$(find data/raw -name "*.jsonl" -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "0")
          echo "records_after=$RECORD_COUNT" >> $GITHUB_OUTPUT
          NEW_RECORDS=$((RECORD_COUNT - ${{ steps.count_before.outputs.records_before }}))
          echo "new_records=$NEW_RECORDS" >> $GITHUB_OUTPUT
          echo "Records after scraping: $RECORD_COUNT"
          echo "New records added: $NEW_RECORDS"

      - name: Check for changes
        id: check_changes
        run: |
          echo "DEBUG: Checking for changes..."
          echo "Git status:"
          git status --porcelain
          echo ""
          echo "Untracked files in data/raw:"
          find data/raw -type f -name "*.jsonl" | head -10
          echo ""
          echo "Git diff (modified files):"
          git diff --name-only
          echo ""
          echo "Git diff --staged (staged files):"
          git diff --staged --name-only
          echo ""
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No new data to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "New data detected!"
            echo ""
            echo "Changed files:"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit and push comprehensive scraping data
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          DATA_DIR="${{ github.event.inputs.data_dir || 'data/raw' }}"
          NEW_RECORDS="${{ steps.count_after.outputs.new_records }}"

          # Detect current branch
          BRANCH=$(git rev-parse --abbrev-ref HEAD)
          
          git add "$DATA_DIR"
          git commit -m "[Auto-commit] Scraping: $NEW_RECORDS new entries saved [skip ci]"

          RETRIES=10
          SUCCESS=0
          for i in $(seq 1 $RETRIES); do
            git fetch origin "$BRANCH"
            git pull --rebase origin "$BRANCH" || git merge origin/"$BRANCH"
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:$BRANCH && {
              echo "Successfully pushed comprehensive scraping data!"
              SUCCESS=1
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done
          if [ "$SUCCESS" -ne 1 ]; then
            echo "ERROR: Failed to push after $RETRIES attempts."
            exit 1
          fi

  # Job 2: Fast detection of recently changed statuses (configurable, default: last 3 months)
  # This job quickly scans recent pages WITHOUT opening individual URLs to find status changes
  # Much faster than comprehensive scraping when only looking for status updates
  recent-entries-detector-with-changed-status:
    runs-on: ubuntu-latest
    name: "2. Recent status change detector"
    outputs:
      changed_urls_count: ${{ steps.detect_changes.outputs.changed_count }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt

      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Detect recently changed statuses
        id: detect_changes
        run: |
          CUTOFF_MONTHS="${{ github.event.inputs.cutoff_months || '3' }}"
          
          echo "RECENT STATUS CHANGE DETECTION (FAST SCAN)"
          echo "Strategy: Scan only listing pages for status changes in the last $CUTOFF_MONTHS months"
          echo "Cutoff months: $CUTOFF_MONTHS"
          
          # Run fast status change detection
          python ./scripts/scrape_data.py \
            --fetch-changed-urls \
            --cutoff-months $CUTOFF_MONTHS
          
          # Count detected URLs
          if [ -f "recent_changed_urls.txt" ]; then
            CHANGED_COUNT=$(wc -l < recent_changed_urls.txt)
          else
            CHANGED_COUNT=0
          fi
          
          echo "changed_count=$CHANGED_COUNT" >> $GITHUB_OUTPUT
          echo "Detected $CHANGED_COUNT recently changed statuses"

      - name: Upload recent changed URLs
        if: steps.detect_changes.outputs.changed_count != '0'
        uses: actions/upload-artifact@v4
        with:
          name: recent-changed-urls
          path: recent_changed_urls.txt
          retention-days: 1

  # Job 3: Extract old pending URLs (older than cutoff_months) from database
  # This job extracts URLs directly from our database for issues that are:
  # 1. Older than the cutoff period
  # 2. Still in a pending state (not resolved, deleted, or rejected)
  # This avoids having to scan thousands of pages for old content
  old-nonresolved-entries-loader:
    runs-on: ubuntu-latest
    name: "3. Old pending URL extractor"
    outputs:
      pending_urls_count: ${{ steps.load_pending.outputs.pending_count }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt

      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Load old pending URLs
        id: load_pending
        run: |
          CUTOFF_MONTHS="${{ github.event.inputs.cutoff_months || '3' }}"
          
          echo "OLD PENDING URL EXTRACTOR"
          echo "Strategy: Extract old pending URLs directly from database (older than $CUTOFF_MONTHS months)"
          echo "Cutoff months: $CUTOFF_MONTHS"
          
          # Load old pending URLs
          python ./scripts/scrape_data.py \
            --load-old-pending \
            --cutoff-months $CUTOFF_MONTHS
          
          # Count pending URLs
          if [ -f "old_pending_urls.txt" ]; then
            PENDING_COUNT=$(wc -l < old_pending_urls.txt)
          else
            PENDING_COUNT=0
          fi
          
          echo "pending_count=$PENDING_COUNT" >> $GITHUB_OUTPUT
          echo "Found $PENDING_COUNT old pending URLs"

      - name: Upload old pending URLs
        if: steps.load_pending.outputs.pending_count > 0
        uses: actions/upload-artifact@v4
        with:
          name: old-pending-urls
          path: old_pending_urls.txt
          retention-days: 1

  # Job 4: Process recently changed statuses (detected by Job 2) to update resolution data
  recent-resolution-date-scraper:
    runs-on: ubuntu-latest
    name: "4. Recent resolution date scraper"
    needs: recent-entries-detector-with-changed-status
    if: needs.recent-entries-detector-with-changed-status.outputs.changed_urls_count > 0
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt

      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Download recent changed URLs
        uses: actions/download-artifact@v5
        with:
          name: recent-changed-urls

      - name: Scrape recent changed URLs
        run: |
          echo "RECENT RESOLUTION DATE SCRAPER"
          echo "Strategy: Targeted resolution date extraction for recently changed statuses detected by Job 2"
          echo "URLs to scrape: ${{ needs.recent-entries-detector-with-changed-status.outputs.changed_urls_count }}"
          
          # Scrape the recently changed URLs
          python ./scripts/scrape_data.py \
            --scrape-urls-file recent_changed_urls.txt

      - name: Check for changes
        id: check_changes
        run: |
          echo "DEBUG: Checking for changes..."
          echo "Git status:"
          git status --porcelain
          echo ""
          echo "Untracked files in data/raw:"
          find data/raw -type f -name "*.jsonl" | head -10
          echo ""
          echo "Git diff (modified files):"
          git diff --name-only
          echo ""
          echo "Git diff --staged (staged files):"
          git diff --staged --name-only
          echo ""
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No recent changes to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "Recent resolution changes detected!"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit recent resolutions
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          # Detect current branch
          BRANCH=$(git rev-parse --abbrev-ref HEAD)
          
          git add data/raw
          git commit -m "[Auto-commit] Scraped recently changed entry statuses [skip ci]"

          # Push with retry logic
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git fetch origin "$BRANCH"
            git pull --rebase origin "$BRANCH" || git merge origin/"$BRANCH"
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:$BRANCH && {
              echo "Successfully pushed recent resolution updates!"
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done

  # Job 5: Check old pending URLs (extracted by Job 3) for resolution updates
  old-resolution-date-scraper:
    runs-on: ubuntu-latest
    name: "5. Old resolution date scraper"
    needs: old-nonresolved-entries-loader
    if: needs.old-nonresolved-entries-loader.outputs.pending_urls_count > 0
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}

      - name: Set up Python & Minimal Dependencies (Scraper-only)
        run: |
          python -m pip install --upgrade pip
          echo "Installing minimal scraper dependencies only (no ML packages)..."
          pip install -r requirements-scraper.txt

      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PWD/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Download old pending URLs
        uses: actions/download-artifact@v5
        with:
          name: old-pending-urls

      - name: Fetch URLs of old unfinished (pending) entries
        run: |
          echo "SCRAPING RESOLUTION STATE OF OLD ISSUES"
          echo "Strategy: Random sampling of old pending issues to check for resolution_date and status change"
          echo "Total URLs available: ${{ needs.old-nonresolved-entries-loader.outputs.pending_urls_count }}"
                    
          # Sample 300 random URLs from the most recent entries (up to 5000)
          if [ -f "old_pending_urls.txt" ]; then
            TOTAL_URLS=$(wc -l < old_pending_urls.txt)
            if [ "$TOTAL_URLS" -gt 5000 ]; then
              # If more than 5000 URLs, sample from the last 5000 entries
              tail -n 5000 old_pending_urls.txt | shuf 2>/dev/null | head -300 > old_pending_urls_sample.txt
              echo "Sampling 300 URLs from the last 5000 entries (total: $TOTAL_URLS)"
            else
              # If 5000 or fewer URLs, sample from all available
              SAMPLE_SIZE=$(( TOTAL_URLS < 300 ? TOTAL_URLS : 300 ))
              shuf old_pending_urls.txt 2>/dev/null | head -$SAMPLE_SIZE > old_pending_urls_sample.txt
              echo "Sampling $SAMPLE_SIZE URLs from all $TOTAL_URLS available entries"
            fi
            # Verify uniqueness of sampled URLs
            UNIQUE_COUNT=$(sort old_pending_urls_sample.txt | uniq | wc -l)
            TOTAL_SAMPLED=$(wc -l < old_pending_urls_sample.txt)
            if [ "$UNIQUE_COUNT" -ne "$TOTAL_SAMPLED" ]; then
              echo "⚠️ Warning: Found $(($TOTAL_SAMPLED - $UNIQUE_COUNT)) duplicate URLs in sample, deduplicating..."
              sort old_pending_urls_sample.txt | uniq > old_pending_urls_clean.txt
              mv old_pending_urls_clean.txt old_pending_urls_sample.txt
            fi
            mv old_pending_urls_sample.txt old_pending_urls.txt
            echo "Processing $(wc -l < old_pending_urls.txt) URLs"
          fi

          # Scrape the old pending URLs
          python ./scripts/scrape_data.py \
            --scrape-urls-file old_pending_urls.txt

      - name: Check for changes
        id: check_changes
        run: |
          echo "DEBUG: Checking for changes..."
          echo "Git status:"
          git status --porcelain
          echo ""
          echo "Untracked files in data/raw:"
          find data/raw -type f -name "*.jsonl" | head -10
          echo ""
          echo "Git diff (modified files):"
          git diff --name-only
          echo ""
          echo "Git diff --staged (staged files):"
          git diff --staged --name-only
          echo ""
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "changes_detected=false" >> $GITHUB_OUTPUT
            echo "No old pending changes to commit"
          else
            echo "changes_detected=true" >> $GITHUB_OUTPUT
            echo "Old pending resolution changes detected!"
            git diff --name-only
          fi

      - name: Configure Git
        if: steps.check_changes.outputs.changes_detected == 'true'
        run: |
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"

      - name: Commit old resolution_dates
        if: steps.check_changes.outputs.changes_detected == 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          # Set current active branch
          BRANCH="$(git rev-parse --abbrev-ref HEAD)"

          git add data/raw
          git commit -m "[Auto-commit] Update of resolution_date of old entries: rnd sample of 250 URLs checked [skip ci]"
          
          # Push with retry logic
          RETRIES=10
          for i in $(seq 1 $RETRIES); do
            git fetch origin "$BRANCH"
            git pull --rebase origin "$BRANCH" || git merge origin/"$BRANCH"
            git push https://x-access-token:$PAT_TOKEN@github.com/${{ github.repository }} HEAD:$BRANCH && {
              echo "Successfully pushed old resolution updates!"
              break
            } || {
              echo "Push failed, retrying ($i/$RETRIES)..."
              sleep 3
            }
          done

  # Job 6: Report results after all jobs complete
  pipeline-summary:
    runs-on: ubuntu-latest
    name: "6. Pipeline Summary"
    needs: [new-entries-scraping, recent-entries-detector-with-changed-status, old-nonresolved-entries-loader, recent-resolution-date-scraper, old-resolution-date-scraper]
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repo
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ secrets.PAT_TOKEN }}
          ref: master

      - name: Report pipeline summary
        run: |
          echo "Járokelő Tracker Pipeline Summary"
          echo "==============================================="
          echo "Data directory structure:"
          ls -la data/raw/ || echo "No data directory found"
          echo ""
          echo "Total JSONL files:"
          find data/raw -name "*.jsonl" -type f | wc -l || echo "0"
          echo ""
          echo "Record counts by month:"
          for file in data/raw/*.jsonl; do
            if [ -f "$file" ]; then
              count=$(wc -l < "$file" 2>/dev/null || echo "0")
              echo "  $(basename "$file"): $count records"
            fi
          done
          echo ""
          echo "Pipeline completed at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
